{
    "fbresnet1qekahdb52": {
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "fbresnet152": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/fbresnet152-2e20f6b4.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "bninception": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/bn_inception-52deb4733.pth",
        "input_space": "BGR",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            255
        ],
        "mean": [
            104,
            117,
            128
        ],
        "std": [
            1,
            1,
            1
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnext101_32x4d": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/resnext101_32x4d-29e315fa.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnext101_64x4d": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/resnext101_64x4d-e77a0586.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "inceptionv4": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            299,
            299
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "inceptionresnetv2": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/inceptionresnetv2-520b38e4.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            299,
            299
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "alexnet": {
        "url": "https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "densenet121": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/densenet121-fbdb23505.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "densenet169": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/densenet169-f470b90a4.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "densenet201": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/densenet201-5750cbb1e.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "densenet161": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/densenet161-347e6b360.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnet18": {
        "url": "https://download.pytorch.org/models/resnet18-5c106cde.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnet34": {
        "url": "https://download.pytorch.org/models/resnet34-333f7ec4.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnet50": {
        "url": "https://download.pytorch.org/models/resnet50-19c8e357.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnet101": {
        "url": "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "resnet152": {
        "url": "https://download.pytorch.org/models/resnet152-b121ed2d.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "inceptionv3": {
        "url": "https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            299,
            299
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "squeezenet1_0": {
        "url": "https://download.pytorch.org/models/squeezenet1_0-a815701f.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "squeezenet1_1": {
        "url": "https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg11": {
        "url": "https://download.pytorch.org/models/vgg11-bbd30ac9.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg11_bn": {
        "url": "https://download.pytorch.org/models/vgg11_bn-6002323d.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg13": {
        "url": "https://download.pytorch.org/models/vgg13-c768596a.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg13_bn": {
        "url": "https://download.pytorch.org/models/vgg13_bn-abd245e5.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg16": {
        "url": "https://download.pytorch.org/models/vgg16-397923af.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg16_bn": {
        "url": "https://download.pytorch.org/models/vgg16_bn-6c64b313.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg19_bn": {
        "url": "https://download.pytorch.org/models/vgg19_bn-c79401a0.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "vgg19": {
        "url": "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "nasnetamobile": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/nasnetamobile-7e03cead.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "nasnetalarge": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            331,
            331
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn68": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn68-4af7d88d2.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn68b": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn68b_extra-363ab9c19.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn92": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn92_extra-fda993c95.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn98": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn98-722954780.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn131": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn131-7af84be88.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "dpn107": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/dpn107_extra-b7f9f4cc9.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.48627450980392156,
            0.4588235294117647,
            0.40784313725490196
        ],
        "std": [
            0.23482446870963955,
            0.23482446870963955,
            0.23482446870963955
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "xception": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/xception-43020ad28.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            299,
            299
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "scale": 0.8975,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "senet154": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/senet154-c7b49a05.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "se_resnet50": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/se_resnet50-ce0d4300.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "se_resnet101": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/se_resnet101-7e38fcc6.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "se_resnet152": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/se_resnet152-d17c99b7.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "se_resnext50_32x4d": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/se_resnext50_32x4d-a260b3a4.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "se_resnext101_32x4d": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "cafferesnet101": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/cafferesnet101-9d633cc0.pth",
        "input_space": "BGR",
        "input_size": [
            3,
            224,
            224
        ],
        "input_range": [
            0,
            255
        ],
        "mean": [
            102.9801,
            115.9465,
            122.7717
        ],
        "std": [
            1,
            1,
            1
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "pnasnet5large": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/pnasnet5large-bf079911.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            331,
            331
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.5,
            0.5,
            0.5
        ],
        "std": [
            0.5,
            0.5,
            0.5
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    },
    "polynet": {
        "url": "http://data.lip6.fr/cadene/pretrainedmodels/polynet-f71d82a5.pth",
        "input_space": "RGB",
        "input_size": [
            3,
            331,
            331
        ],
        "input_range": [
            0,
            1
        ],
        "mean": [
            0.485,
            0.456,
            0.406
        ],
        "std": [
            0.229,
            0.224,
            0.225
        ],
        "num_classes": 1000,
        "origin": "pretrainedmodels",
        "parameters": "default",
        "task": "categorization reproduction",
        "trainingSet": "imagenet"
    }
}